.\" Man page generated from reStructuredText.
.
.TH "CLUSTEROR" "1" "Dec 02, 2016" "0.0.post0.dev61+nfa78ece.dirty" "clusteror"
.SH NAME
clusteror \- clusteror 0.0.post0.dev61+nfa78ece.dirty
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.sp
This is the documentation of \fBclusteror\fP\&.
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
This is the main page of your project\(aqs \fI\%Sphinx\fP
documentation. It is formatted in \fI\%reStructuredText\fP\&. Add additional pages by creating
rst\-files in \fBdocs\fP and adding them to the \fI\%toctree\fP below. Use then
\fI\%references\fP in order to link
them from this page, e.g. \fI\%authors\fP and \fI\%Changes in Sphinx\fP\&.
.sp
It is also possible to refer to the documentation of other Python packages
with the \fI\%Python domain syntax\fP\&. By default you
can reference the documentation of \fI\%Sphinx\fP,
\fI\%Python\fP, \fI\%NumPy\fP, \fI\%SciPy\fP, \fI\%matplotlib\fP, \fI\%Pandas\fP, \fI\%Scikit\-Learn\fP\&. You can add more by
extending the \fBintersphinx_mapping\fP in your Sphinx\(aqs \fBconf.py\fP\&.
.sp
The pretty useful extension \fI\%autodoc\fP is activated by
default and lets you include documentation from docstrings. Docstrings can
be written in \fI\%Google\fP
(recommended!), \fI\%NumPy\fP
and \fI\%classical\fP
style.
.UNINDENT
.UNINDENT
.SH CONTENTS
.SS Modules
.SS clusteror package
.SS Submodules
.SS clusteror.core module
.sp
This module contains \fBClusteror\fP class capsulating raw data to discover
clusters from, the cleaned data for a clusteror to run on.
.sp
The clustering model encompasses two parts:
.INDENT 0.0
.IP 1. 3
Neural network:
Pre\-training (often encountered in Deep Learning context)
is implemented to achieve a goal that the neural network maps the input
data of higher dimension to a one dimensional representation. Ideally this
mapping is one\-to\-one.
A Denoising Autoencoder (DA) or Stacked Denoising Autoencoder (SDA) is
implemented for this purpose.
.IP 2. 3
One dimensional clustering model:
A separate model segments the samples against the one dimensional
representation. Two models are available in this class definition:
.INDENT 3.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
K\-Means
.IP \(bu 2
Valley model
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.sp
The pivot idea here is given the neural network is a good one\-to\-one mapper
the separate clustering model on one dimensional representation is equivalent
to a clustering model on the original high dimensional data.
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
Valley model is explained in details in module \fBclusteror.utils\fP\&.
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class clusteror.core.Clusteror(raw_data)
Bases: \fI\%object\fP
.sp
\fBClusteror\fP class can train neural networks \fIDA\fP or
\fISDA\fP, train taggers, or load saved models
from files.
.INDENT 7.0
.TP
.B Parameters
\fBraw_data\fP (\fIPandas DataFrame\fP) \-\- Dataframe read from data source. It can be original dataset without
any preprocessing or with a certain level of manipulation for
future analysis.
.UNINDENT
.INDENT 7.0
.TP
.B _raw_data
\fIPandas DataFrame\fP \-\- Stores the original dataset. It\(aqs the dataset that later
post\-clustering performance analysis will be based on.
.UNINDENT
.INDENT 7.0
.TP
.B _cleaned_data
\fIPandas DataFrame\fP \-\- Preprocessed data. Not necessarily has same number of columns with
\fB_raw_data\fP as a categorical column can derive multiple columns.
As the \fBtanh\fP function is used as activation function for symmetric
consideration. All columns should have values in range \fB[\-1, 1]\fP,
otherwise an \fBOutRangeError\fP will be raised.
.UNINDENT
.INDENT 7.0
.TP
.B _network
\fIstr\fP \-\- \fBda\fP for \fIDA\fP; \fBsda\fP for \fISDA\fP\&.
Facilating functions called with one or the other algorithm.
.UNINDENT
.INDENT 7.0
.TP
.B _da_dim_reducer
\fITheano function\fP \-\- Keeps the Theano function that is from trained DA model. Reduces
the dimension of the cleaned data down to one.
.UNINDENT
.INDENT 7.0
.TP
.B _sda_dim_reducer
\fITheano function\fP \-\- Keeps the Theano function that is from trained SDA model. Reduces
the dimension of the cleaned data down to one.
.UNINDENT
.INDENT 7.0
.TP
.B _one_dim_data
\fINumpy Array\fP \-\- The dimension reduced one dimensional data.
.UNINDENT
.INDENT 7.0
.TP
.B _valley
\fIPython function\fP \-\- Trained valley model tagging sample with their one dimensional
representation.
.UNINDENT
.INDENT 7.0
.TP
.B _kmeans
\fIScikit\-Learn K\-Means model\fP \-\- Trained K\-Means model tagging samples with their one dimensional
representation.
.UNINDENT
.INDENT 7.0
.TP
.B _tagger
\fIstr\fP \-\- Keeps records of which tagger implemented.
.UNINDENT
.INDENT 7.0
.TP
.B _field_importance
\fIList\fP \-\- Keeps the list of coefficiences that influence the clustering
emphasis.
.UNINDENT
.INDENT 7.0
.TP
.B add_cluster()
Tags each sample regarding their reduced one dimensional value. Adds
an extra column \fB\(aqcluster\(aq\fP to \fBraw_data\fP, seggesting a
zero\-based cluster ID.
.UNINDENT
.INDENT 7.0
.TP
.B cleaned_data
\fIPandas DataFrame\fP \-\- For assgining cleaned dataframe to \fB_cleaned_dat\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B da_dim_reducer
\fITheano function\fP \-\- Function that reduces dataset dimension. Attribute
\fB_network\fP is given \fBda\fP to designate the method of the
autoencoder as \fBDA\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B field_importance
\fIList\fP \-\- Significance that given to fields when training of neural
network is done. Fields with a large number will be given more
attention.
.sp
\fBNOTE:\fP
.INDENT 7.0
.INDENT 3.5
The importance is only meaningful relatively between fields. If no
values are specified, all fields are treated equally.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B Parameters
\fBfield_importance\fP (\fIList or Dict, default None (List of Ones)\fP) \-\- .INDENT 7.0
.IP \(bu 2
If a list is designated, all fields should be assigned an
.UNINDENT
.sp
importance, viz, the length of the list should be equal to the
length of the features training the neural network.
.INDENT 7.0
.IP \(bu 2
It can also be given in a dict. In such a case, the fields can
.UNINDENT
.sp
be selectively given a value. Dict key is for field name and value
is for the importance. Fields not included will be initiated with
the default value one. A warning will be issued when a key is
not on the list of field names, mostly because of a typo.

.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B classmethod from_csv(filepath, **kwargs)
Class method for directly reading CSV file.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- Path to the CSV file
.IP \(bu 2
\fB**kwargs\fP (\fIkeyword arguments\fP) \-\- Other keyword arguments passed to \fBpandas.read_csv\fP
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B kmeans
\fIPython function\fP \-\- Trained on the dimension reduced one dimensional
data that segregates subjects into concentration of existence in a
subset of \fB[\-1, 1]\fP with K\-Means algorithm.  \fB_tagger\fP is
given \fBvalley\fP to facilitate follow\-up usages.
.UNINDENT
.INDENT 7.0
.TP
.B load_dim_reducer(filepath=\(aqdim_reducer.pk\(aq)
Loads saved dimension reducer. Need to first name the network type.
.INDENT 7.0
.TP
.B Parameters
\fBfilepath\fP (\fI\%str\fP) \-\- 
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B load_kmeans(filepath)
Loads a saved K\-Means tagger from a file.
.INDENT 7.0
.TP
.B filepath: str
File path to the file saving the K\-Means tagger.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B load_valley(filepath)
Loads a saved valley tagger from a file. Create the valley function
from the saved parameters.
.INDENT 7.0
.TP
.B filepath: str
File path to the file saving the valley tagger.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B one_dim_data
\fINumpy Array\fP \-\- Stores the output of neural network that has dimension
one.
.UNINDENT
.INDENT 7.0
.TP
.B raw_data
\fIPandas DataFrame\fP \-\- For assgining new values to \fB_raw_data\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B reduce_to_one_dim()
Reduces the dimension of input dataset to one before the tagging
in the next step.
.sp
Input of the Theano function is the cleaned data and output is a
one dimensional data stored in \fB_one_dim_data\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B save_dim_reducer(filepath=\(aqdim_reducer.pk\(aq, include_network=False)
Save dimension reducer from the neural network training.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- Filename to store the dimension reducer.
.IP \(bu 2
\fBinclude_network\fP (\fIboolean\fP) \-\- If true, prefix the filepath with the network type.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B save_kmeans(filepath, include_taggername=False)
Saves K\-Means model to the named file path. Can add a prefix to
indicate this saves a K\-Means model.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- File path for saving the model.
.IP \(bu 2
\fBinclude_taggername\fP (\fIboolean, default False\fP) \-\- Include the \fBkmean_\fP prefix in filename if true.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B save_valley(filepath, include_taggername=False)
Saves valley tagger.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- File path to save the tagger.
.IP \(bu 2
\fBinclude_taggername\fP (\fIboolean, default False\fP) \-\- Include the \fBvalley_\fP prefix in filename if true.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B sda_dim_reducer
\fITheano function\fP \-\- Function that reduces dataset dimension. Attribute
\fB_network\fP is given \fBsda\fP to designate the method of the
autoencoder as \fBSDA\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B tagger
\fIstr\fP \-\- Name the tagger if necessary to do so, which will facilitate, e.g.
prefixing the filepath.
.UNINDENT
.INDENT 7.0
.TP
.B train_da_dim_reducer(field_importance=None, batch_size=50, corruption_level=0.3, learning_rate=0.002, min_epochs=200, patience=60, patience_increase=2, improvement_threshold=0.98, verbose=False)
Trains a \fBDA\fP neural network.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfield_importance\fP (\fIList or Dict, default None (List of Ones)\fP) \-\- .INDENT 2.0
.IP \(bu 2
If a list is designated, all fields should be assigned an
.UNINDENT
.sp
importance, viz, the length of the list should be equal to the
length of the features training the neural network.
.INDENT 2.0
.IP \(bu 2
It can also be given in a dict. In such a case, the fields can
.UNINDENT
.sp
be selectively given a value. Dict key is for field name and value
is for the importance. Fields not included will be initiated with
the default value one. A warning will be issued when a key is
not on the list of field names, mostly because of a typo.

.IP \(bu 2
\fBbatch_size\fP (\fI\%int\fP) \-\- Size of each training batch. Necessary to derive the number
of batches.
.IP \(bu 2
\fBcorruption_level\fP (\fIfloat, between 0 and 1\fP) \-\- Dropout rate in reading input, typical pratice in deep learning
to avoid overfitting.
.IP \(bu 2
\fBlearning_rate\fP (\fI\%float\fP) \-\- Propagating step size for gredient descent algorithm.
.IP \(bu 2
\fBmin_epochs\fP (\fI\%int\fP) \-\- The mininum number of training epoch to run. It can be exceeded
depending on the setup of patience and ad\-hoc training progress.
.IP \(bu 2
\fBpatience\fP (\fI\%int\fP) \-\- True number of training epochs to run if larger than
\fBmin_epochs\fP\&. Note it is potentially increased during the
training if the cost is better than the expectation from
current cost.
.IP \(bu 2
\fBpatience_increase\fP (\fI\%int\fP) \-\- Coefficient used to increase patience against epochs that
have been run.
.IP \(bu 2
\fBimprovement_threshold\fP (\fIfloat, between 0 and 1\fP) \-\- Minimum improvement considered as substantial improvement, i.e.
new cost over existing lowest cost lower than this value.
.IP \(bu 2
\fBverbose\fP (\fIboolean, default False\fP) \-\- Prints out training at each epoch if true.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B train_kmeans(n_clusters=10, **kwargs)
Trains K\-Means model on top of the one dimensional data derived from
dimension reducers.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBn_clusters\fP (\fI\%int\fP) \-\- The number of clusters required to start a K\-Means learning.
.IP \(bu 2
\fB**kwargs\fP (\fIkeyword arguments\fP) \-\- Any other keyword arguments passed on to Scikit\-Learn K\-Means
model.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B train_sda_dim_reducer(field_importance=None, batch_size=50, hidden_layers_sizes=[20], corruption_levels=[0.3], learning_rate=0.002, min_epochs=200, patience=60, patience_increase=2, improvement_threshold=0.98, verbose=False)
Trains a \fBSDA\fP neural network.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfield_importance\fP (\fIList or Dict, default None (List of Ones)\fP) \-\- .INDENT 2.0
.IP \(bu 2
If a list is designated, all fields should be assigned an
.UNINDENT
.sp
importance, viz, the length of the list should be equal to the
length of the features training the neural network.
.INDENT 2.0
.IP \(bu 2
It can also be given in a dict. In such a case, the fields can
.UNINDENT
.sp
be selectively given a value. Dict key is for field name and value
is for the importance. Fields not included will be initiated with
the default value one. A warning will be issued when a key is
not on the list of field names, mostly because of a typo.

.IP \(bu 2
\fBbatch_size\fP (\fI\%int\fP) \-\- Size of each training batch. Necessary to derive the number
of batches.
.IP \(bu 2
\fBhidden_layers_sizes\fP (\fIList of ints\fP) \-\- Number of neurons in the hidden layers (all but the input layer).
.IP \(bu 2
\fBcorruption_levels\fP (\fIList of floats, between 0 and 1\fP) \-\- Dropout rate in reading input, typical pratice in deep learning
to avoid overfitting.
.IP \(bu 2
\fBlearning_rate\fP (\fI\%float\fP) \-\- Propagating step size for gredient descent algorithm.
.IP \(bu 2
\fBmin_epochs\fP (\fI\%int\fP) \-\- The mininum number of training epoch to run. It can be exceeded
depending on the setup of patience and ad\-hoc training progress.
.IP \(bu 2
\fBpatience\fP (\fI\%int\fP) \-\- True number of training epochs to run if larger than
\fBmin_epochs\fP\&. Note it is potentially increased during the
training if the cost is better than the expectation from
current cost.
.IP \(bu 2
\fBpatience_increase\fP (\fI\%int\fP) \-\- Coefficient used to increase patience against epochs that
have been run.
.IP \(bu 2
\fBimprovement_threshold\fP (\fIfloat, between 0 and 1\fP) \-\- Minimum improvement considered as substantial improvement, i.e.
new cost over existing lowest cost lower than this value.
.IP \(bu 2
\fBverbose\fP (\fIboolean, default False\fP) \-\- Prints out training at each epoch if true.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B train_valley(bins=100, contrast=0.3)
Trains the ability to cut the universe of samples into clusters based
how the dimension reduced dataset assembles in a histogram. Unlike
the K\-Means, no need to preset the number of clusters.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBbins\fP (\fI\%int\fP) \-\- Number of bins to aggregate the one dimensional data.
.IP \(bu 2
\fBcontrast\fP (\fIfloat, between 0 and 1\fP) \-\- Threshold used to define local minima and local maxima. Detailed
explanation in \fButils.find_local_extremes\fP\&.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B valley
\fIPython function\fP \-\- Trained on the dimension reduced one dimensional
data that segregates subjects into concentration of existence in a
subset of \fB[\-1, 1]\fP, by locating the "valley" in the distribution
landscape. \fB_tagger\fP is given \fBvalley\fP to facilitate
follow\-up usages.
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B exception clusteror.core.OutRangeError
Bases: \fI\%Exception\fP
.sp
Exceptions thrown as cleaned data go beyond range \fB[\-1, 1]\fP\&.
.UNINDENT
.SS clusteror.data_layer module
.INDENT 0.0
.TP
.B class clusteror.data_layer.DataStore
Bases: \fI\%object\fP
.INDENT 7.0
.TP
.B cleaned_data
.UNINDENT
.INDENT 7.0
.TP
.B id_col_name
.UNINDENT
.INDENT 7.0
.TP
.B raw_data
.UNINDENT
.INDENT 7.0
.TP
.B transactions
.UNINDENT
.UNINDENT
.SS clusteror.discovery_layer module
.SS clusteror.implementation_layer module
.SS clusteror.mlp module
.SS clusteror.nn module
.sp
This module comprises of classes for neural networks.
.INDENT 0.0
.TP
.B class clusteror.nn.SdA(n_ins, hidden_layers_sizes, np_rs=None, theano_rs=None, field_importance=None, input_data=None)
Bases: \fI\%object\fP
.sp
Stacked Denoising Autoencoder (SDA) class.
.sp
A SdA model is obtained by stacking several DAs.
The hidden layer of the dA at layer \fIi\fP becomes the input of
the dA at layer \fIi+1\fP\&. The first layer dA gets as input the input of
the SdA, and the hidden layer of the last dA represents the output.
Note that after pretraining, the SdA is dealt with as a normal MLP,
the dAs are only used to initialize the weights.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBn_ins\fP (\fI\%int\fP) \-\- Input dimension.
.IP \(bu 2
\fBhidden_layers_sizes\fP (\fIlist of int\fP) \-\- Each int will be assgined to each hidden layer. Same number of hidden
layers will be created.
.IP \(bu 2
\fBnp_rs\fP (\fINumpy function\fP) \-\- Numpy random state.
.IP \(bu 2
\fBtheano_rs\fP (\fITheano function\fP) \-\- Theano random generator that gives symbolic random values.
.IP \(bu 2
\fBfield_importance\fP (\fIlist or Numpy array\fP) \-\- Put on each field when calculating the cost.  If not given,
all fields given equal weight ones.
.IP \(bu 2
\fBinput_data\fP (\fITheano symbolic variable\fP) \-\- Variable for input data.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B theano_rs
\fITheano function\fP \-\- Theano random generator that gives symbolic random values.
.UNINDENT
.INDENT 7.0
.TP
.B field_importance
\fIlist or Numpy array\fP \-\- Put on each field when calculating the cost.  If not given,
all fields given equal weight ones.
.UNINDENT
.INDENT 7.0
.TP
.B W
\fITheano shared variable\fP \-\- Weight matrix. Dimension (n_visible, n_hidden).
.UNINDENT
.INDENT 7.0
.TP
.B W_prime
\fITheano shared variable\fP \-\- Transposed weight matrix. Dimension (n_hidden, n_visible).
.UNINDENT
.INDENT 7.0
.TP
.B bhid
\fITheano shared variable\fP \-\- Bias on output side. Dimension n_hidden.
.UNINDENT
.INDENT 7.0
.TP
.B bvis
\fITheano shared variable\fP \-\- Bias on input side. Dimension n_visible.
.UNINDENT
.INDENT 7.0
.TP
.B x
\fITheano symbolic variable\fP \-\- Used as input to build graph.
.UNINDENT
.INDENT 7.0
.TP
.B params
\fIlist\fP \-\- List packs neural network paramters.
.UNINDENT
.INDENT 7.0
.TP
.B dA_layers
\fIlist\fP \-\- List that keeps dA instances.
.UNINDENT
.INDENT 7.0
.TP
.B n_layers
\fIint\fP \-\- Number of hidden layers, len(dA_layers).
.UNINDENT
.INDENT 7.0
.TP
.B get_final_hidden_layer(input_data)
Computes the values of the last hidden layer.
.INDENT 7.0
.TP
.B Parameters
\fBinput_data\fP (\fITheano symbolic variable\fP) \-\- Data input to neural network.
.TP
.B Returns
A graph with output as the hidden layer values.
.TP
.B Return type
Theano graph
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B get_first_reconstructed_input(hidden)
Computes the reconstructed input given the values of the last
hidden layer.
.INDENT 7.0
.TP
.B Parameters
\fBhidden\fP (\fITheano symbolic variable\fP) \-\- Data input to neural network at the hidden layer side.
.TP
.B Returns
A graph with output as the reconstructed data at the visible side.
.TP
.B Return type
Theano graph
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B pretraining_functions(train_set, batch_size)
This function computes the cost and the updates for one trainng
step of the dA.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBtrain_set\fP (\fITheano shared variable\fP) \-\- The complete training dataset.
.IP \(bu 2
\fBbatch_size\fP (\fI\%int\fP) \-\- Number of rows for each mini\-batch.
.UNINDENT
.TP
.B Returns
Theano functions that run one step training on each dA layers.
.TP
.B Return type
List
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class clusteror.nn.dA(n_visible, n_hidden, np_rs=None, theano_rs=None, field_importance=None, initial_W=None, initial_bvis=None, initial_bhid=None, input_data=None)
Bases: \fI\%object\fP
.sp
Denoising Autoencoder (DA) class.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBn_visible\fP (\fI\%int\fP) \-\- Input dimension.
.IP \(bu 2
\fBn_hidden\fP (\fI\%int\fP) \-\- Output dimension.
.IP \(bu 2
\fBnp_rs\fP (\fINumpy function\fP) \-\- Numpy random state.
.IP \(bu 2
\fBtheano_rs\fP (\fITheano function\fP) \-\- Theano random generator that gives symbolic random values.
.IP \(bu 2
\fBfield_importance\fP (\fIlist or Numpy array\fP) \-\- Put on each field when calculating the cost.  If not given,
all fields given equal weight ones.
.IP \(bu 2
\fBinitial_W\fP (\fINumpy matrix\fP) \-\- Initial weight matrix. Dimension (n_visible, n_hidden).
.IP \(bu 2
\fBinitial_bvis\fP (\fINumpy array\fP) \-\- Initial bias on input side. Dimension n_visible.
.IP \(bu 2
\fBinitial_bhid\fP (\fINumpy arry\fP) \-\- Initial bias on output side. Dimension n_hidden.
.IP \(bu 2
\fBinput_data\fP (\fITheano symbolic variable\fP) \-\- Variable for input data.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B theano_rs
\fITheano function\fP \-\- Theano random generator that gives symbolic random values.
.UNINDENT
.INDENT 7.0
.TP
.B field_importance
\fIlist or Numpy array\fP \-\- Put on each field when calculating the cost.  If not given,
all fields given equal weight ones.
.UNINDENT
.INDENT 7.0
.TP
.B W
\fITheano shared variable\fP \-\- Weight matrix. Dimension (n_visible, n_hidden).
.UNINDENT
.INDENT 7.0
.TP
.B W_prime
\fITheano shared variable\fP \-\- Transposed weight matrix. Dimension (n_hidden, n_visible).
.UNINDENT
.INDENT 7.0
.TP
.B bhid
\fITheano shared variable\fP \-\- Bias on output side. Dimension n_hidden.
.UNINDENT
.INDENT 7.0
.TP
.B bvis
\fITheano shared variable\fP \-\- Bias on input side. Dimension n_visible.
.UNINDENT
.INDENT 7.0
.TP
.B x
\fITheano symbolic variable\fP \-\- Used as input to build graph.
.UNINDENT
.INDENT 7.0
.TP
.B params
\fIlist\fP \-\- List packs neural network paramters.
.UNINDENT
.INDENT 7.0
.TP
.B get_corrupted_input(input_data, corruption_level)
Corrupts the input by multiplying input with an array of zeros and
ones that is generated by binomial trials.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBinput_data\fP (\fITheano symbolic variable\fP) \-\- Data input to neural network.
.IP \(bu 2
\fBcorruption_level\fP (\fIfloat or Theano symbolic variable\fP) \-\- Probability to corrupt a bit in the input data. Between 0 and 1.
.UNINDENT
.TP
.B Returns
A graph with output as the corrupted input.
.TP
.B Return type
Theano graph
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B get_cost_updates(corruption_level, learning_rate)
This function computes the cost and the updates for one trainng
step of the dA.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBcorruption_level\fP (\fIfloat or Theano symbolic variable\fP) \-\- Probability to corrupt a bit in the input data. Between 0 and 1.
.IP \(bu 2
\fBlearning_rate\fP (\fIfloat or Theano symbolic variable\fP) \-\- Step size for Gradient Descent algorithm.
.UNINDENT
.TP
.B Returns
.INDENT 7.0
.IP \(bu 2
\fBcost\fP (\fITheano graph\fP) \-\- A graph with output as the cost.
.IP \(bu 2
\fBupdates\fP (\fIList of tuples\fP) \-\- Instructions of how to update parameters. Used in training stage
to update parameters.
.UNINDENT

.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B get_hidden_values(input_data)
Computes the values of the hidden layer.
.INDENT 7.0
.TP
.B Parameters
\fBinput_data\fP (\fITheano symbolic variable\fP) \-\- Data input to neural network.
.TP
.B Returns
A graph with output as the hidden layer values.
.TP
.B Return type
Theano graph
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B get_reconstructed_input(hidden)
Computes the reconstructed input given the values of the
hidden layer.
.INDENT 7.0
.TP
.B Parameters
\fBhidden\fP (\fITheano symbolic variable\fP) \-\- Data input to neural network at the hidden layer side.
.TP
.B Returns
A graph with output as the reconstructed data at the visible side.
.TP
.B Return type
Theano graph
.UNINDENT
.UNINDENT
.UNINDENT
.SS clusteror.plot module
.sp
Plotting tools relevant for illustrating and comparing clustering results
can be found in this module.
.INDENT 0.0
.TP
.B clusteror.plot.group_occurance_plot(one_dim_data, cat_label, labels, group_label, colors=None, figsize=(10, 6), bbox_to_anchor=(1.01, 1), loc=2, grid=True, show=True, filepath=None, **kwargs)
Plot the distribution of a one dimensional \fBordinal or categorical\fP data
in a bar chart. This tool is useful to check the clustering impact in this
one\-dimensional sub\-space.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBone_dim_data\fP (\fIlist, Pandas Series, Numpy Array, or any iterable\fP) \-\- A sequence of data. Each element if for an instance.
.IP \(bu 2
\fBcat_label\fP (\fI\%str\fP) \-\- Field name will be used for the one dimensional data.
.IP \(bu 2
\fBlabels\fP (\fIlist, Pandas Series, Numpy Array, or any iterable\fP) \-\- The segment label for each sample in one_dim_data.
.IP \(bu 2
\fBgroup_label\fP (\fI\%str\fP) \-\- Field name will be used for the cluster ID.
.IP \(bu 2
\fBcolors\fP (\fIlist, default None\fP) \-\- Colours for each category existing in this one dimensional data.
Default colour scheme used if not supplied.
.IP \(bu 2
\fBfigsize\fP (\fI\%tuple\fP) \-\- Figure size (width, height).
.IP \(bu 2
\fBbbox_to_anchor\fP (\fI\%tuple\fP) \-\- Instruction to placing the legend box relative to the axes. Details
refer to \fBMatplotlib\fP document.
.IP \(bu 2
\fBloc\fP (\fI\%int\fP) \-\- The corner of the legend box to anchor. Details refer to \fBMatplotlib\fP
document.
.IP \(bu 2
\fBgrid\fP (\fIboolean, default True\fP) \-\- Show grid.
.IP \(bu 2
\fBshow\fP (\fIboolean, default True\fP) \-\- Show figure in pop\-up windows if true. Save to files if False.
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- File name to saving the plot. Must be assigned a valid filepath if
\fBshow\fP is False.
.IP \(bu 2
\fB**kwargs\fP (\fIkeyword arguments\fP) \-\- Other keyword arguemnts passed on to \fBmatplotlib.pyplot.scatter\fP\&.
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 7.0
.INDENT 3.5
Instances in a same cluster does not necessarily assemble together in
all one dimensional sub\-spaces. There can be possibly no clustering
capaility for certain features. Additionally certain features play a
secondary role in clustering as having less importance in
\fBfield_importance\fP in \fBclusteror\fP module.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B clusteror.plot.hist_plot_one_dim_group_data(one_dim_data, labels, bins=11, colors=None, figsize=(10, 6), bbox_to_anchor=(1.01, 1), loc=2, grid=True, show=True, filepath=None, **kwargs)
Plot the distribution of a one dimensional numerical data in a histogram.
This tool is useful to check the clustering impact in this one\-dimensional
sub\-space.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBone_dim_data\fP (\fIlist, Pandas Series, Numpy Array, or any iterable\fP) \-\- A sequence of data. Each element if for an instance.
.IP \(bu 2
\fBlabels\fP (\fIlist, Pandas Series, Numpy Array, or any iterable\fP) \-\- The segment label for each sample in \fBone_dim_data\fP\&.
.IP \(bu 2
\fBbins\fP (\fIint or iterable\fP) \-\- If an integer, bins \- 1 bins created or a list of the delimiters.
.IP \(bu 2
\fBcolors\fP (\fIlist, default None\fP) \-\- Colours for each group. Use equally distanced colours on colour map
if not supplied.
.IP \(bu 2
\fBfigsize\fP (\fI\%tuple\fP) \-\- Figure size (width, height).
.IP \(bu 2
\fBbbox_to_anchor\fP (\fI\%tuple\fP) \-\- Instruction to placing the legend box relative to the axes. Details
refer to \fBMatplotlib\fP document.
.IP \(bu 2
\fBloc\fP (\fI\%int\fP) \-\- The corner of the legend box to anchor. Details refer to \fBMatplotlib\fP
document.
.IP \(bu 2
\fBgrid\fP (\fIboolean, default True\fP) \-\- Show grid.
.IP \(bu 2
\fBshow\fP (\fIboolean, default True\fP) \-\- Show figure in pop\-up windows if true. Save to files if False.
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- File name to saving the plot. Must be assigned a valid filepath if
\fBshow\fP is False.
.IP \(bu 2
\fB**kwargs\fP (\fIkeyword arguments\fP) \-\- Other keyword arguemnts passed on to \fBmatplotlib.pyplot.scatter\fP\&.
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 7.0
.INDENT 3.5
Instances in a same cluster does not necessarily assemble together in
all one dimensional sub\-spaces. There can be possibly no clustering
capaility for certain features. Additionally certain features play a
secondary role in clustering as having less importance in
\fBfield_importance\fP in \fBclusteror\fP module.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B clusteror.plot.scatter_plot_two_dim_group_data(two_dim_data, labels, markers=None, colors=None, figsize=(10, 6), xlim=None, ylim=None, alpha=0.8, bbox_to_anchor=(1.01, 1), loc=2, grid=True, show=True, filepath=None, **kwargs)
Plot the distribution of a two dimensional data against clustering groups
in a scatter plot.
.sp
A point represents an instance in the dataset. Points in a same cluster
are painted with a same colour.
.sp
This tool is useful to check the clustering impact in this two\-dimensional
sub\-space.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBtwo_dim_data\fP (\fIPandas DataFrame\fP) \-\- A dataframe with two columns. The first column goes to the x\-axis,
and the second column goes to the y\-axis.
.IP \(bu 2
\fBlabels\fP (\fIlist, Pandas Series, Numpy Array, or any iterable\fP) \-\- The segment label for each sample in \fBtwo_dim_data\fP\&.
.IP \(bu 2
\fBmarkers\fP (\fI\%list\fP) \-\- Marker names for each group.
.IP \(bu 2
\fBbbox_to_anchor\fP (\fI\%tuple\fP) \-\- Instruction to placing the legend box relative to the axes. Details
refer to \fBMatplotlib\fP document.
.IP \(bu 2
\fBcolors\fP (\fIlist, default None\fP) \-\- Colours for each group. Use equally distanced colours on colour map
if not supplied.
.IP \(bu 2
\fBfigsize\fP (\fI\%tuple\fP) \-\- Figure size (width, height).
.IP \(bu 2
\fBxlim\fP (\fI\%tuple\fP) \-\- X\-axis limits.
.IP \(bu 2
\fBylim\fP (\fI\%tuple\fP) \-\- Y\-axis limits.
.IP \(bu 2
\fBalpha\fP (\fIfloat, between 0 and 1\fP) \-\- Marker transparency. From 0 to 1: from transparent to opaque.
.IP \(bu 2
\fBloc\fP (\fI\%int\fP) \-\- The corner of the legend box to anchor. Details refer to \fBMatplotlib\fP
document.
.IP \(bu 2
\fBgrid\fP (\fIboolean, default True\fP) \-\- Show grid.
.IP \(bu 2
\fBshow\fP (\fIboolean, default True\fP) \-\- Show figure in pop\-up windows if true. Save to files if False.
.IP \(bu 2
\fBfilepath\fP (\fI\%str\fP) \-\- File name to saving the plot. Must be assigned a valid filepath if
\fBshow\fP is False.
.IP \(bu 2
\fB**kwargs\fP (\fIkeyword arguments\fP) \-\- Other keyword arguemnts passed on to \fBmatplotlib.pyplot.scatter\fP\&.
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 7.0
.INDENT 3.5
Instances in a same cluster does not necessarily assemble together in
all two dimensional sub\-spaces. There can be possibly no clustering
capaility for certain features. Additionally certain features play a
secondary role in clustering as having less importance in
\fBfield_importance\fP in \fBclusteror\fP module.
.UNINDENT
.UNINDENT
.UNINDENT
.SS clusteror.preprocessing_layer module
.SS clusteror.settings module
.SS clusteror.skeleton module
.sp
This is a skeleton file that can serve as a starting point for a Python
console script. To run this script uncomment the following line in the
entry_points section in setup.cfg:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.TP
.B console_scripts =
fibonacci = clusteror.skeleton:run
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Then run \fIpython setup.py install\fP which will install the command \fIfibonacci\fP
inside your current environment.
Besides console scripts, the header (i.e. until _logger...) of this file can
also be used as template for Python modules.
.sp
Note: This skeleton file can be safely removed if not needed!
.INDENT 0.0
.TP
.B clusteror.skeleton.fib(n)
Fibonacci example function
.INDENT 7.0
.TP
.B Parameters
\fBn\fP (\fI\%int\fP) \-\- integer
.TP
.B Returns
n\-th Fibonacci number
.TP
.B Return type
\fI\%int\fP
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B clusteror.skeleton.main(args)
Main entry point allowing external calls
.INDENT 7.0
.TP
.B Parameters
\fBargs\fP (\fI[str]\fP) \-\- command line parameter list
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B clusteror.skeleton.parse_args(args)
Parse command line parameters
.INDENT 7.0
.TP
.B Parameters
\fBargs\fP (\fI[str]\fP) \-\- command line parameters as list of strings
.TP
.B Returns
command line parameters namespace
.TP
.B Return type
\fI\%argparse.Namespace\fP
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B clusteror.skeleton.run()
Entry point for console_scripts
.UNINDENT
.INDENT 0.0
.TP
.B clusteror.skeleton.setup_logging(loglevel)
Setup basic logging
.INDENT 7.0
.TP
.B Parameters
\fBloglevel\fP (\fI\%int\fP) \-\- minimum loglevel for emitting messages
.UNINDENT
.UNINDENT
.SS clusteror.utils module
.sp
This module works as a transient store of useful functions. New standalone
functions will be first placed here. As they grow in number and can be
consolidated into an independent class, module, or even a new package.
.INDENT 0.0
.TP
.B clusteror.utils.find_local_extremes(series, contrast)
Finds local minima and maxima according to \fBcontrast\fP\&. In theory,
they can be determined by first derivative and second derivative. The
result derived this way is of no value in dealing with a very noisy,
zig\-zag data as too many local extremes would be found for any turn\-around.
The method presented here compares the point currently looked at and the
opposite potential extreme that is updated as scanning through the
data sequence. For instance, a potential maximum is 10, then a data point
of value smaller than 10 / (1 + contrast) is written down as a local
minimum.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBseries\fP (\fIPandas Series\fP) \-\- One dimenional data to find local extremes in.
.IP \(bu 2
\fBcontrast\fP (\fI\%float\fP) \-\- A value between 0 and 1 as a threshold between minimum and maximum.
.UNINDENT
.TP
.B Returns
.INDENT 7.0
.IP \(bu 2
\fBlocal_min_inds\fP (\fIlist\fP) \-\- List of indices for local minima.
.IP \(bu 2
\fBlocal_mins\fP (\fIlist\fP) \-\- List of minimum values.
.IP \(bu 2
\fBlocal_max_inds\fP (\fIlist\fP) \-\- List of indices for local maxima.
.IP \(bu 2
\fBlocal_maxs\fP (\fIlist\fP) \-\- List of maximum values.
.UNINDENT

.UNINDENT
.UNINDENT
.SS Module contents
.SS tests package
.SS Submodules
.SS tests.conftest module
.sp
Dummy conftest.py for clusteror.
.sp
If you don\(aqt know what this is for, just leave it empty.
Read more about conftest.py under:
\fI\%https://pytest.org/latest/plugins.html\fP
.SS tests.example_iris module
.SS tests.example_tips module
.SS tests.test_clusteror module
.INDENT 0.0
.TP
.B class tests.test_clusteror.TestDA(methodName=\(aqrunTest\(aq)
Bases: \fBunittest.case.TestCase\fP
.INDENT 7.0
.TP
.B setUp()
.UNINDENT
.INDENT 7.0
.TP
.B test_dA_conrrupted_input()
.UNINDENT
.INDENT 7.0
.TP
.B test_dA_cost()
.UNINDENT
.INDENT 7.0
.TP
.B test_dA_hidden_values()
.UNINDENT
.INDENT 7.0
.TP
.B test_dA_reconstructed_input()
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class tests.test_clusteror.TestSdA(methodName=\(aqrunTest\(aq)
Bases: \fBunittest.case.TestCase\fP
.INDENT 7.0
.TP
.B setUp()
.UNINDENT
.INDENT 7.0
.TP
.B test_SdA_final_hidden_layer()
.UNINDENT
.INDENT 7.0
.TP
.B test_SdA_first_reconstructed_layer()
.UNINDENT
.INDENT 7.0
.TP
.B test_SdA_pretraining_functions()
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B tests.test_clusteror.tanh_cross_entropy(field_importance, dat_in, dat_rec)
.UNINDENT
.SS tests.test_skeleton module
.INDENT 0.0
.TP
.B tests.test_skeleton.test_fib()
.UNINDENT
.SS Module contents
.SH INDICES AND TABLES
.INDENT 0.0
.IP \(bu 2
genindex
.IP \(bu 2
modindex
.IP \(bu 2
search
.UNINDENT
.SH COPYRIGHT
2016, Fei Zhan
.\" Generated by docutils manpage writer.
.
