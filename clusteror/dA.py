import numpy as np
import theano
from theano import tensor as T
from theano import shared
from theano.tensor.shared_randomstreams import RandomStreams
from .settings import numpy_random_seed
from .settings import theano_random_seed


class dA(object):
    """
    Denoising Auto-Encoder class (dA).
    """
    def __init__(self, n_visible, n_hidden,
                 np_rs=None, theano_rs=None, field_importance=None,
                 initial_W=None, initial_bvis=None,
                 initial_bhid=None, input_data=None):
        '''
        n_visible: int
           Input dimension.
        n_hidden: int
          Output dimension.
        np_rs:  Numpy random state
        theano_rs:
          Theano random generator that gives symbolic random values.
        field_importance:  list or Numpy array
          Put on each field when calculating the cost.  If not given,
          all fields given equal weight ones.
        initial_W:  Numpy matrix
          Initial weight matrix. Dimension (n_visible, n_hidden).
        initial_bvis: Numpy array
          Initial bias on input side. Dimension n_visible.
        initial_bhid: Numpy arry
          Initial bias on output side. Dimension n_hidden.
        input_data: Theano symbolic variable
          Variable for input data.
        '''
        if np_rs is None:
            np_rs = np.random.RandomState(numpy_random_seed)
        # set theano random state if not given
        if theano_rs is None:
            theano_rs = RandomStreams(np_rs.randint(theano_random_seed))
        self.theano_rs = theano_rs
        # set equal field weights if not given
        if not field_importance:
            field_importance = np.ones(n_visible, dtype=theano.config.floatX)
        else:
            field_importance = np.asarray(
                field_importance,
                dtype=theano.config.floatX
            )
        # store in a shared variable
        self.field_importance = shared(
            value=field_importance,
            name='field_importance',
            borrow=True
        )
        # note : W' was written as `W_prime` and b' as `b_prime`
        if initial_W is None:
            # W is initialized with `initial_W` which is uniformely sampled
            # from -4*sqrt(6./(n_visible+n_hidden)) and
            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if
            # converted using asarray to dtype
            # theano.config.floatX so that the code is runable on GPU
            initial_W = np.asarray(
                np_rs.uniform(
                    low=-4 * np.sqrt(6. / (n_hidden + n_visible)),
                    high=4 * np.sqrt(6. / (n_hidden + n_visible)),
                    size=(n_visible, n_hidden)
                ),
                dtype=theano.config.floatX
            )
        self.W = shared(value=initial_W, name='W', borrow=True)
        # tied weights, therefore W_prime is W transpose
        self.W_prime = self.W.T
        if initial_bvis is None:
            initial_bvis = np.zeros(n_visible, dtype=theano.config.floatX)
        # b_prime corresponds to the bias of the visible
        self.bhid_prime = shared(
            value=initial_bvis,
            name='bhid_prime',
            borrow=True
        )
        if initial_bhid is None:
            initial_bhid = np.zeros(n_hidden, dtype=theano.config.floatX)
        # b corresponds to the bias of the hidden
        self.bhid = shared(value=initial_bhid, name='bhid', borrow=True)
        # if no input_data is given, generate a variable representing the input
        if input_data is None:
            # we use a matrix because we expect a minibatch of several
            # examples, each example being a row
            self.x = T.matrix(name='input_data')
        else:
            self.x = input_data
        self.params = [self.W, self.bhid, self.bhid_prime]

    def get_corrupted_input(self, input_data, corruption_level):
        '''
        Corrupts the input by multiplying input with an array of zeros and
        ones that is generated by binomial trials.
        '''
        corrupted_input = self.theano_rs.binomial(
            size=input_data.shape,
            n=1,
            p=1 - corruption_level,
            dtype=theano.config.floatX
        ) * input_data
        return corrupted_input

    def get_hidden_values(self, input_data):
        """
        Computes the values of the hidden layer.
        """
        return T.tanh(T.dot(input_data, self.W) + self.bhid)

    def get_reconstructed_input(self, hidden):
        """
        Computes the reconstructed input given the values of the
        hidden layer.
        """
        return T.tanh(T.dot(hidden, self.W_prime) + self.bhid_prime)

    def get_cost_updates(self, corruption_level, learning_rate):
        """
        This function computes the cost and the updates for one trainng
        step of the dA.
        """
        tilde_x = self.get_corrupted_input(self.x, corruption_level)
        y = self.get_hidden_values(tilde_x)
        z = self.get_reconstructed_input(y)
        # need this cross entropy because now the x and z are in the
        # range [-1, 1]
        L = - T.sum(
            self.field_importance * (
                .5 * (1 + self.x) * T.log(.5 * (1 + z)) +
                .5 * (1 - self.x) * T.log(.5 * (1 - z))
            ),
            axis=1
        )
        # note : L is now a vector, where each element is the
        #        cross-entropy cost of the reconstruction of the
        #        corresponding example of the minibatch. We need to
        #        compute the average of all these to get the cost of
        #        the minibatch
        cost = T.mean(L)
        # compute the gradients of the cost of the `dA` with respect
        # to its parameters
        gparams = T.grad(cost, self.params)
        # generate the list of updates
        updates = [
            (param, param - learning_rate * gparam)
            for param, gparam in zip(self.params, gparams)
        ]
        return cost, updates
